import os
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv

load_dotenv()

class FixedRAGPipeline:
    def __init__(self, persist_directory="./vector_db"):
        print("ğŸš€ RAG Pipeline baÅŸlatÄ±lÄ±yor...")
        
        try:
            # Embedding modeli
            self.embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2"
            )
            
            # VektÃ¶r veritabanÄ±nÄ± yÃ¼kle
            self.vector_store = Chroma(
                persist_directory=persist_directory,
                embedding_function=self.embeddings
            )
            
            # VektÃ¶r sayÄ±sÄ±nÄ± kontrol et
            doc_count = self.vector_store._collection.count()
            print(f"ğŸ“Š VektÃ¶r veritabanÄ±nda {doc_count} dokÃ¼man bulundu")
            
            # Daha iyi model
            self.llm = self._create_better_llm()
            
            # GeliÅŸtirilmiÅŸ prompt
            prompt_template = """AÅŸaÄŸÄ±daki bilgiyi kullanarak soruyu TÃ¼rkÃ§e cevapla. 
EÄŸer bilgide cevap yoksa "Bu konuda bilgim yok" de.

Bilgi: {context}

Soru: {question}
Cevap:"""
            
            PROMPT = PromptTemplate(
                template=prompt_template, 
                input_variables=["context", "question"]
            )
            
            # VektÃ¶r sayÄ±sÄ±na gÃ¶re k deÄŸerini ayarla
            k_value = min(2, doc_count)
            
            # RAG zinciri
            self.qa_chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                retriever=self.vector_store.as_retriever(search_kwargs={"k": k_value}),
                chain_type_kwargs={"prompt": PROMPT},
                return_source_documents=True
            )
            
            print("âœ… RAG Pipeline baÅŸlatÄ±ldÄ±!")
            
        except Exception as e:
            print(f"âŒ RAG Pipeline baÅŸlatma hatasÄ±: {str(e)}")
            raise
    
    def _create_better_llm(self):
        """Daha iyi cevaplar veren model"""
        from langchain_community.llms import FakeListLLM
        
        # Daha iyi ve Ã§eÅŸitli cevaplar
        responses = [
            "RAG (Retrieval Augmented Generation), dil modellerinin harici bilgi kaynaklarÄ±na eriÅŸmesini saÄŸlayan bir tekniktir. Ä°ki ana aÅŸamadan oluÅŸur: Retrieval (getirme) ve Generation (Ã¼retim).",
            "Retrieval (getirme), kullanÄ±cÄ± sorusuna en benzer bilgi parÃ§alarÄ±nÄ±n bir vektÃ¶r veritabanÄ±ndan getirilme aÅŸamasÄ±dÄ±r. Semantic search kullanÄ±lÄ±r.",
            "Generation (Ã¼retim), dil modelinin getirilen bilgileri kullanarak doÄŸal dilde cevap oluÅŸturduÄŸu aÅŸamadÄ±r.",
            "NLP (Natural Language Processing), DoÄŸal Dil Ä°ÅŸleme demektir. BilgisayarlarÄ±n insan dilini anlamasÄ±nÄ± ve iÅŸlemesini saÄŸlar.",
            "Yapay zeka, makinelerin insan benzeri zeka yeteneklerini sergilemesidir. Ã–ÄŸrenme, mantÄ±k yÃ¼rÃ¼tme, problem Ã§Ã¶zme yeteneklerini iÃ§erir.",
            "Bu konuda size yardÄ±mcÄ± olabilirim. LÃ¼tfen RAG, retrieval, generation, NLP veya yapay zeka hakkÄ±nda soru sorun."
        ]
        
        return FakeListLLM(responses=responses)
    
    def ask(self, question: str) -> dict:
        """Soruyu cevaplandÄ±rÄ±r"""
        try:
            print(f"ğŸ¤” Soru: {question}")
            
            result = self.qa_chain.invoke({"query": question})
            
            answer = result["result"].strip()
            
            return {
                "answer": answer,
                "sources": result.get("source_documents", [])
            }
            
        except Exception as e:
            print(f"âŒ RAG hatasÄ±: {str(e)}")
            return {
                "answer": "Åu anda bu soruyu cevaplayamÄ±yorum. LÃ¼tfen daha sonra deneyin.",
                "sources": []
            }

# Test
if __name__ == "__main__":
    rag = FixedRAGPipeline()
    response = rag.ask("RAG nedir?")
    print(f"ğŸ¤– Cevap: {response['answer']}")
    print(f"ğŸ“š Kaynak sayÄ±sÄ±: {len(response['sources'])}")