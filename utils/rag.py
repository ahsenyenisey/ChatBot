import os
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import FakeListLLM

class FixedRAGPipeline:
    def __init__(self, persist_directory="./vector_db"):
        print("ğŸš€ RAG Pipeline baÅŸlatÄ±lÄ±yor...")
        
        try:
            # Embedding modeli
            self.embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2"
            )
            
            # VektÃ¶r veritabanÄ±nÄ± yÃ¼kle
            self.vector_store = Chroma(
                persist_directory=persist_directory,
                embedding_function=self.embeddings
            )
            
            # VektÃ¶r sayÄ±sÄ±nÄ± kontrol et
            doc_count = self.vector_store._collection.count()
            print(f"ğŸ“Š VektÃ¶r veritabanÄ±nda {doc_count} dokÃ¼man bulundu")
            
            # Basit LLM
            self.llm = FakeListLLM(responses=[
                "RAG (Retrieval Augmented Generation), dil modellerinin harici bilgi kaynaklarÄ±na eriÅŸmesini saÄŸlayan bir tekniktir.",
                "Retrieval aÅŸamasÄ±nda, kullanÄ±cÄ± sorusuna en benzer bilgi parÃ§alarÄ± vektÃ¶r veritabanÄ±ndan getirilir.",
                "Generation aÅŸamasÄ±nda, dil modeli getirilen bilgileri kullanarak doÄŸal dilde cevap oluÅŸturur.",
                "NLP (Natural Language Processing), DoÄŸal Dil Ä°ÅŸleme anlamÄ±na gelir.",
                "Yapay zeka, makinelerin insan benzeri zeka yeteneklerini sergilemesidir.",
            ])
            
            print("âœ… RAG Pipeline baÅŸlatÄ±ldÄ±!")
            
        except Exception as e:
            print(f"âŒ RAG Pipeline baÅŸlatma hatasÄ±: {str(e)}")
            raise
    
    def ask(self, question: str) -> dict:
        """Basit soru-cevap"""
        try:
            print(f"ğŸ¤” Soru: {question}")
            
            # Basit benzerlik arama
            docs = self.vector_store.similarity_search(question, k=2)
            
            # Basit cevap oluÅŸturma
            if docs:
                answer = self.llm.invoke(question)
                return {
                    "answer": answer,
                    "sources": docs
                }
            else:
                return {
                    "answer": "Bu konuda bilgim yok.",
                    "sources": []
                }
            
        except Exception as e:
            print(f"âŒ RAG hatasÄ±: {str(e)}")
            return {
                "answer": "Åu anda bu soruyu cevaplayamÄ±yorum. LÃ¼tfen daha sonra deneyin.",
                "sources": []
            }

if __name__ == "__main__":
    rag = FixedRAGPipeline()
    response = rag.ask("RAG nedir?")
    print(f"ğŸ¤– Cevap: {response['answer']}")
    print(f"ğŸ“š Kaynak sayÄ±sÄ±: {len(response['sources'])}")