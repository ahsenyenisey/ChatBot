# # # utils/rag_fixed.py
# # import os
# # from langchain_community.vectorstores import Chroma
# # from langchain_community.embeddings import HuggingFaceEmbeddings
# # from langchain.chains import RetrievalQA
# # from langchain.prompts import PromptTemplate
# # from dotenv import load_dotenv

# # load_dotenv()

# # class FixedRAGPipeline:
# #     def __init__(self, persist_directory="./vector_db"):
# #         print("ğŸš€ DÃœZELTÄ°LMÄ°Å RAG Pipeline baÅŸlatÄ±lÄ±yor...")
        
# #         try:
# #             # Embedding modeli
# #             self.embeddings = HuggingFaceEmbeddings(
# #                 model_name="sentence-transformers/all-MiniLM-L6-v2"
# #             )
            
# #             # VektÃ¶r veritabanÄ±nÄ± yÃ¼kle
# #             self.vector_store = Chroma(
# #                 persist_directory=persist_directory,
# #                 embedding_function=self.embeddings
# #             )
            
# #             # VektÃ¶r sayÄ±sÄ±nÄ± kontrol et
# #             doc_count = self.vector_store._collection.count()
# #             print(f"ğŸ“Š VektÃ¶r veritabanÄ±nda {doc_count} dokÃ¼man bulundu")
            
# #             # Basit ve gÃ¼venilir model
# #             self.llm = self._create_smart_llm()
            
# #             # Basit prompt
# #             prompt_template = """AÅŸaÄŸÄ±daki bilgiye gÃ¶re soruyu cevapla:

# # {context}

# # Soru: {question}
# # Cevap:"""
            
# #             PROMPT = PromptTemplate(
# #                 template=prompt_template, 
# #                 input_variables=["context", "question"]
# #             )
            
# #             # VektÃ¶r sayÄ±sÄ±na gÃ¶re k deÄŸerini ayarla
# #             k_value = min(2, doc_count)  # Maksimum 2, ama vektÃ¶r sayÄ±sÄ±ndan fazla olamaz
            
# #             # RAG zinciri
# #             self.qa_chain = RetrievalQA.from_chain_type(
# #                 llm=self.llm,
# #                 chain_type="stuff",
# #                 retriever=self.vector_store.as_retriever(search_kwargs={"k": k_value}),
# #                 chain_type_kwargs={"prompt": PROMPT},
# #                 return_source_documents=True
# #             )
            
# #             print("âœ… RAG Pipeline baÅŸarÄ±yla baÅŸlatÄ±ldÄ±!")
            
# #         except Exception as e:
# #             print(f"âŒ RAG Pipeline baÅŸlatma hatasÄ±: {str(e)}")
# #             raise
    
# #     def _create_smart_llm(self):
# #         """AkÄ±llÄ± basit model - KESÄ°N Ã‡ALIÅIR"""
# #         from langchain_community.llms import FakeListLLM
        
# #         smart_responses = [
# #             "RAG (Retrieval Augmented Generation), dil modellerinin harici bilgi kaynaklarÄ±na eriÅŸmesini saÄŸlayan bir tekniktir.",
# #             "Retrieval (getirme), kullanÄ±cÄ± sorusuna en benzer bilgi parÃ§alarÄ±nÄ±n bir vektÃ¶r veritabanÄ±ndan getirilme aÅŸamasÄ±dÄ±r.",
# #             "Generation (Ã¼retim), dil modelinin getirilen bilgileri kullanarak cevap oluÅŸturduÄŸu aÅŸamadÄ±r.",
# #             "NLP, DoÄŸal Dil Ä°ÅŸleme (Natural Language Processing) demektir.",
# #             "Yapay zeka, makinelerin insan benzeri zeka yeteneklerini sergilemesidir.",
# #             "Bu konuda size yardÄ±mcÄ± olabilirim. LÃ¼tfen RAG, retrieval, generation veya NLP hakkÄ±nda soru sorun."
# #         ]
        
# #         return FakeListLLM(responses=smart_responses)
    
# #     def ask(self, question: str) -> dict:
# #         """Soruyu cevaplandÄ±rÄ±r"""
# #         try:
# #             print(f"ğŸ¤” Soru: {question}")
            
# #             if not question.strip():
# #                 return {
# #                     "answer": "LÃ¼tfen bir soru sorun.",
# #                     "sources": []
# #                 }
            
# #             result = self.qa_chain.invoke({"query": question})
            
# #             answer = result["result"].strip()
            
# #             return {
# #                 "answer": answer,
# #                 "sources": result.get("source_documents", [])
# #             }
            
# #         except Exception as e:
# #             print(f"âŒ RAG hatasÄ±: {str(e)}")
# #             return {
# #                 "answer": "Bu konuda size yardÄ±mcÄ± olabilirim.",
# #                 "sources": []
# #             }

# # # Test fonksiyonu
# # if __name__ == "__main__":
# #     try:
# #         rag = FixedRAGPipeline()
        
# #         print("ğŸ§ª KESÄ°N Ã‡ALIÅAN TEST BAÅLIYOR...")
# #         test_questions = [
# #             "RAG nedir?",
# #             "Retrieval ne demek?",
# #             "Generation aÅŸamasÄ± nedir?",
# #             "Bu dokÃ¼man ne hakkÄ±nda?",
# #             "NLP nedir?",
# #             "Yapay zeka nedir?"
# #         ]
        
# #         for i, test_question in enumerate(test_questions, 1):
# #             print(f"\n{'='*50}")
# #             print(f"ğŸ”¹ Test {i}: '{test_question}'")
# #             response = rag.ask(test_question)
# #             print(f"ğŸ¤– Cevap: {response['answer']}")
# #             print(f"ğŸ“š Kaynak SayÄ±sÄ±: {len(response['sources'])}")
            
# #             if response["sources"]:
# #                 print("ğŸ“– Kaynak Ã¶nizleme:")
# #                 for j, source in enumerate(response["sources"], 1):
# #                     content = source.page_content[:150] + "..." if len(source.page_content) > 150 else source.page_content
# #                     print(f"   {j}. {content}")
            
# #     except Exception as e:
# #         print(f"âŒ Test baÅŸarÄ±sÄ±z: {e}")

# # utils/rag.py - GÃœNCELLENMÄ°Å VERSÄ°YON

# # utils/rag.py - TEK VE TEMÄ°Z VERSÄ°YON
# import os
# from langchain_community.vectorstores import Chroma
# from langchain_community.embeddings import HuggingFaceEmbeddings
# from langchain.chains import RetrievalQA
# from langchain.prompts import PromptTemplate
# from dotenv import load_dotenv

# load_dotenv()

# class FixedRAGPipeline:
#     def __init__(self, persist_directory="./vector_db"):
#         print("ğŸš€ RAG Pipeline baÅŸlatÄ±lÄ±yor...")
        
#         try:
#             # Embedding modeli
#             self.embeddings = HuggingFaceEmbeddings(
#                 model_name="sentence-transformers/all-MiniLM-L6-v2"
#             )
            
#             # VektÃ¶r veritabanÄ±nÄ± yÃ¼kle
#             self.vector_store = Chroma(
#                 persist_directory=persist_directory,
#                 embedding_function=self.embeddings
#             )
            
#             # VektÃ¶r sayÄ±sÄ±nÄ± kontrol et
#             doc_count = self.vector_store._collection.count()
#             print(f"ğŸ“Š VektÃ¶r veritabanÄ±nda {doc_count} dokÃ¼man bulundu")
            
#             # Basit ve gÃ¼venilir model
#             self.llm = self._create_simple_llm()
            
#             # Basit prompt
#             prompt_template = """AÅŸaÄŸÄ±daki bilgiye gÃ¶re soruyu cevapla:

# {context}

# Soru: {question}
# Cevap:"""
            
#             PROMPT = PromptTemplate(
#                 template=prompt_template, 
#                 input_variables=["context", "question"]
#             )
            
#             # VektÃ¶r sayÄ±sÄ±na gÃ¶re k deÄŸerini ayarla
#             k_value = min(2, doc_count)
            
#             # RAG zinciri
#             self.qa_chain = RetrievalQA.from_chain_type(
#                 llm=self.llm,
#                 chain_type="stuff",
#                 retriever=self.vector_store.as_retriever(search_kwargs={"k": k_value}),
#                 chain_type_kwargs={"prompt": PROMPT},
#                 return_source_documents=True
#             )
            
#             print("âœ… RAG Pipeline baÅŸlatÄ±ldÄ±!")
            
#         except Exception as e:
#             print(f"âŒ RAG Pipeline baÅŸlatma hatasÄ±: {str(e)}")
#             raise
    
#     def _create_simple_llm(self):
#         """Basit ve gÃ¼venilir model"""
#         from langchain_community.llms import FakeListLLM
        
#         # Sabit cevaplar
#         responses = [
#             "RAG (Retrieval Augmented Generation), dil modellerinin harici bilgi kaynaklarÄ±na eriÅŸmesini saÄŸlayan bir tekniktir.",
#             "Retrieval (getirme), kullanÄ±cÄ± sorusuna en benzer bilgi parÃ§alarÄ±nÄ±n bir vektÃ¶r veritabanÄ±ndan getirilme aÅŸamasÄ±dÄ±r.",
#             "Generation (Ã¼retim), dil modelinin getirilen bilgileri kullanarak cevap oluÅŸturduÄŸu aÅŸamadÄ±r.",
#             "NLP (Natural Language Processing), DoÄŸal Dil Ä°ÅŸleme demektir.",
#             "Yapay zeka, makinelerin insan benzeri zeka yeteneklerini sergilemesidir."
#         ]
        
#         return FakeListLLM(responses=responses)
    
#     def ask(self, question: str) -> dict:
#         """Soruyu cevaplandÄ±rÄ±r"""
#         try:
#             print(f"ğŸ¤” Soru: {question}")
            
#             result = self.qa_chain.invoke({"query": question})
            
#             return {
#                 "answer": result["result"].strip(),
#                 "sources": result.get("source_documents", [])
#             }
            
#         except Exception as e:
#             print(f"âŒ RAG hatasÄ±: {str(e)}")
#             return {
#                 "answer": "Bu konuda size yardÄ±mcÄ± olabilirim.",
#                 "sources": []
#             }

# # Test
# if __name__ == "__main__":
#     rag = FixedRAGPipeline()
#     response = rag.ask("RAG nedir?")
#     print(f"ğŸ¤– Cevap: {response['answer']}")

# utils/rag.py - GÃœNCELLENMÄ°Å
import os
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv

load_dotenv()

class FixedRAGPipeline:
    def __init__(self, persist_directory="./vector_db"):
        print("ğŸš€ RAG Pipeline baÅŸlatÄ±lÄ±yor...")
        
        try:
            # Embedding modeli
            self.embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2"
            )
            
            # VektÃ¶r veritabanÄ±nÄ± yÃ¼kle
            self.vector_store = Chroma(
                persist_directory=persist_directory,
                embedding_function=self.embeddings
            )
            
            # VektÃ¶r sayÄ±sÄ±nÄ± kontrol et
            doc_count = self.vector_store._collection.count()
            print(f"ğŸ“Š VektÃ¶r veritabanÄ±nda {doc_count} dokÃ¼man bulundu")
            
            # Daha iyi model
            self.llm = self._create_better_llm()
            
            # GeliÅŸtirilmiÅŸ prompt
            prompt_template = """AÅŸaÄŸÄ±daki bilgiyi kullanarak soruyu TÃ¼rkÃ§e cevapla. 
EÄŸer bilgide cevap yoksa "Bu konuda bilgim yok" de.

Bilgi: {context}

Soru: {question}
Cevap:"""
            
            PROMPT = PromptTemplate(
                template=prompt_template, 
                input_variables=["context", "question"]
            )
            
            # VektÃ¶r sayÄ±sÄ±na gÃ¶re k deÄŸerini ayarla
            k_value = min(2, doc_count)
            
            # RAG zinciri
            self.qa_chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                retriever=self.vector_store.as_retriever(search_kwargs={"k": k_value}),
                chain_type_kwargs={"prompt": PROMPT},
                return_source_documents=True
            )
            
            print("âœ… RAG Pipeline baÅŸlatÄ±ldÄ±!")
            
        except Exception as e:
            print(f"âŒ RAG Pipeline baÅŸlatma hatasÄ±: {str(e)}")
            raise
    
    def _create_better_llm(self):
        """Daha iyi cevaplar veren model"""
        from langchain_community.llms import FakeListLLM
        
        # Daha iyi ve Ã§eÅŸitli cevaplar
        responses = [
            "RAG (Retrieval Augmented Generation), dil modellerinin harici bilgi kaynaklarÄ±na eriÅŸmesini saÄŸlayan bir tekniktir. Ä°ki ana aÅŸamadan oluÅŸur: Retrieval (getirme) ve Generation (Ã¼retim).",
            "Retrieval (getirme), kullanÄ±cÄ± sorusuna en benzer bilgi parÃ§alarÄ±nÄ±n bir vektÃ¶r veritabanÄ±ndan getirilme aÅŸamasÄ±dÄ±r. Semantic search kullanÄ±lÄ±r.",
            "Generation (Ã¼retim), dil modelinin getirilen bilgileri kullanarak doÄŸal dilde cevap oluÅŸturduÄŸu aÅŸamadÄ±r.",
            "NLP (Natural Language Processing), DoÄŸal Dil Ä°ÅŸleme demektir. BilgisayarlarÄ±n insan dilini anlamasÄ±nÄ± ve iÅŸlemesini saÄŸlar.",
            "Yapay zeka, makinelerin insan benzeri zeka yeteneklerini sergilemesidir. Ã–ÄŸrenme, mantÄ±k yÃ¼rÃ¼tme, problem Ã§Ã¶zme yeteneklerini iÃ§erir.",
            "Bu konuda size yardÄ±mcÄ± olabilirim. LÃ¼tfen RAG, retrieval, generation, NLP veya yapay zeka hakkÄ±nda soru sorun."
        ]
        
        return FakeListLLM(responses=responses)
    
    def ask(self, question: str) -> dict:
        """Soruyu cevaplandÄ±rÄ±r"""
        try:
            print(f"ğŸ¤” Soru: {question}")
            
            result = self.qa_chain.invoke({"query": question})
            
            answer = result["result"].strip()
            
            return {
                "answer": answer,
                "sources": result.get("source_documents", [])
            }
            
        except Exception as e:
            print(f"âŒ RAG hatasÄ±: {str(e)}")
            return {
                "answer": "Åu anda bu soruyu cevaplayamÄ±yorum. LÃ¼tfen daha sonra deneyin.",
                "sources": []
            }

# Test
if __name__ == "__main__":
    rag = FixedRAGPipeline()
    response = rag.ask("RAG nedir?")
    print(f"ğŸ¤– Cevap: {response['answer']}")
    print(f"ğŸ“š Kaynak sayÄ±sÄ±: {len(response['sources'])}")